# ğŸ•µï¸ Real-time Fraud Detection Pipeline
A production-ready system for detecting fraudulent banking transactions using real-time streaming, batch processing, and machine learning.

## ğŸ“‹ Overview
This project simulates a complete fraud detection pipeline used by financial institutions. It generates realistic transaction data, processes it in real-time using Kafka, trains machine learning models to detect fraud, and provides monitoring through an interactive dashboard.

## ğŸ¯ Key Features
- **Real-time Fraud Scoring:** ML model scores transactions as they flow through Kafka

- **Batch ETL Processing:** Apache Airflow pipeline for data cleaning and model retraining

- **Interactive Monitoring:** Streamlit dashboard with real-time metrics and visualizations

- **Data Warehouse:** PostgreSQL for historical analysis and reporting

- **Containerized Infrastructure:** Docker Compose for Kafka, Zookeeper, and PostgreSQL

## ğŸš€ Quick Start
Prerequisites
- Python 3.9+

- Docker & Docker Compose

- Git

## Installation
**Clone the repository**
```bash
git clone https://github.com/yourusername/fraud-pipeline.git
cd fraud-pipeline
```
**Create virtual environment and install dependencies**
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**Start infrastructure**
```bash
docker-compose up -d
```
**Wait for services to initialize**
```bash
sleep 30
```
**Initialize database**
```bash
 python services/database_setup.py
 ```
## Running the System
Open multiple terminal windows:

**Terminal 1 - Generate Transactions:**
```bash
python simulator/producer.py
```
**Terminal 2 - Real-time Fraud Detection:**
```bash
python services/scorer.py
```
**Terminal 3 - Launch Dashboard:**

```bash
streamlit run dashboard/fraud_dashboard.py --server.port 8501
```
**Terminal 4 - Start Airflow:**

```bash
# Initialize Airflow database
airflow db migrate

# Create admin user
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin

# Start Airflow services
airflow api-server --port 8080 &
airflow scheduler &
```
## ğŸ“Š Architecture
```bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Transaction   â”‚    â”‚   Kafka     â”‚    â”‚   Real-time     â”‚
â”‚   Generator     â”œâ”€â”€â”€â–ºâ”‚   Broker    â”œâ”€â”€â”€â–ºâ”‚   Scoring       â”‚
â”‚   (simulator/)  â”‚    â”‚  (Docker)   â”‚    â”‚  (scripts/)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow ETL   â”‚    â”‚ PostgreSQL  â”‚    â”‚   Dashboard     â”‚
â”‚   (airflow/)    â”œâ”€â”€â”€â–ºâ”‚  (Docker)   â”‚â—„â”€â”€â”€â”¤   (dashboard/)  â”‚
â”‚   Model Trainingâ”‚    â”‚             â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## ğŸ“ Project Structure
```bash
fraud-pipeline/
â”œâ”€â”€ airflow/                    # Apache Airflow workflows
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ etl_clean_data.py  # ETL pipeline DAG
â”œâ”€â”€ dashboard/                  # Streamlit dashboard
â”‚   â””â”€â”€ fraud_dashboard.py     # Real-time monitoring UI
â”œâ”€â”€ data/                       # Data files
â”‚   â”œâ”€â”€ raw.parquet           # Raw transaction data
â”‚   â”œâ”€â”€ cleaned.parquet       # Cleaned data
â”‚   â””â”€â”€ warehouse_staging.csv # PostgreSQL staging file
â”œâ”€â”€ docker-compose.yml          # Container orchestration
â”œâ”€â”€ kafka/                      # Kafka configuration
â”œâ”€â”€ model/                      # Machine Learning models
â”‚   â””â”€â”€ fraud.pkl             # Trained fraud detection model
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”‚   â””â”€â”€ train.ipynb           # Model training notebook
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ consumer.py           # Kafka consumer for real-time scoring
â”‚   â””â”€â”€ database_test.py      # Database connection tests
â”œâ”€â”€ services/                   # Service scripts
â”‚   â”œâ”€â”€ database_setup.py     # Database initialization
â”‚   â””â”€â”€ scorer.py             # Real-time scoring service
â”œâ”€â”€ simulator/                  # Transaction simulation
â”‚   â”œâ”€â”€ generator.py          # Transaction data generator
â”‚   â””â”€â”€ producer.py           # Kafka producer
â”œâ”€â”€ venv/                      # Python virtual environment
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ LICENSE                    # MIT License
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Components Explained
1. **Transaction Simulation (simulator/)**
- generator.py: Creates realistic banking transaction data

- producer.py: Publishes transactions to Kafka topic

2. **Real-time Processing (scripts/, services/)**
- consumer.py: Consumes Kafka messages, applies ML model for fraud detection

- scorer.py: Fraud scoring service using trained model

3. **Batch Processing (airflow/, notebooks/)**
- etl_clean_data.py: Airflow DAG for data cleaning, model training, and loading to PostgreSQL

- train.ipynb: Jupyter notebook for model training and experimentation

4. **Data Storage (services/)**
- database_setup.py: Initializes PostgreSQL database schema

- PostgreSQL: Stores processed transactions for historical analysis

5. **Monitoring & Visualization (dashboard/)**
fraud_dashboard.py: Interactive dashboard showing:

- Real-time transaction metrics

- Fraud detection statistics

- Geographic fraud distribution

- Merchant risk analysis

- Data export capabilities

## ğŸ“ˆ Machine Learning
**Model Training**
The fraud detection model is trained in notebooks/train.ipynb and the pipeline includes:

- Data preprocessing: Cleaning, normalization

- Feature engineering: Transaction amount analysis

- Model training: Random Forest classifier

- Evaluation: Accuracy, precision, recall metrics

**Model Deployment**
- Real-time: Model loaded by Kafka consumer for instant scoring

- Batch: Model retrained hourly via Airflow pipeline

- Persistence: Model saved as model/fraud.pkl

## ğŸ› ï¸ Configuration
Environment Variables
Create a .env file:

```bash
POSTGRES_HOST=localhost
POSTGRES_PORT=5433
POSTGRES_DB=fraud_dw
POSTGRES_USER=admin
POSTGRES_PASSWORD=password
KAFKA_BROKER=localhost:9093
Access Points
Dashboard: http://localhost:8501

Airflow UI: http://localhost:8080 (admin/admin)

PostgreSQL: psql -h localhost -p 5433 -U admin -d fraud_dw

Kafka: Broker at localhost:9093
```

## ğŸ§ª Testing

**Test database connection**
 ```bash
 python scripts/database_test.py
```
**Test Kafka producer/consumer**
```bash
python simulator/producer.py --test
python scripts/consumer.py --test
```

**Trigger Airflow pipeline manually**
```bash
airflow dags trigger fraud_pipeline_v3
```
## ğŸ¤ Contributing
Fork the repository

- Create a feature branch (git checkout -b feature/AmazingFeature)

- Commit your changes (git commit -m 'Add AmazingFeature')

- Push to the branch (git push origin feature/AmazingFeature)

- Open a Pull Request

## ğŸ“„ License
Distributed under the MIT License. See LICENSE for more information.

## ğŸ“¬ Contact
Julius Irungu - ğŸ“§ juligatuna@gmail.com

Project Link: https://github.com/juligatuna/fraud-pipeline

## ğŸ™ Acknowledgments
- Apache Kafka for stream processing

- Apache Airflow for workflow orchestration

- Streamlit for dashboard creation

- PostgreSQL for data warehousing

- Scikit-learn for machine learning